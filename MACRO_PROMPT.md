# BATTLERPG AI - Sistema Agentivo de Desarrollo

## ğŸ“Œ CONTEXTO DEL PROYECTO

EstÃ¡s trabajando en BattleRPG AI, un juego de combate 1v1 estilo PokÃ©mon con IA 
adaptativa basada en Reinforcement Learning. Cada jugador tiene un equipo de 3 
personajes y debe derrotar al equipo rival. El objetivo es crear agentes que 
aprendan del comportamiento del oponente y adapten sus estrategias dinÃ¡micamente.

### Documentos de Referencia
- Propuesta de proyecto en Google Docs
- EstÃ¡ndares de cÃ³digo definidos
- Arquitectura tÃ©cnica a desarrollar

---

## ğŸ¯ OBJETIVOS PRINCIPALES

1. **Sistema de Combate Robusto**: Personajes, equipos, turnos y victoria
2. **MecÃ¡nicas PokÃ©mon-like**: Cambios, ventajas de tipo, habilidades
3. **IA Adaptativa**: Agentes que observan y aprenden de patrones adversariales
4. **Reinforcement Learning**: ImplementaciÃ³n de Q-Learning/DQN para aprendizaje
5. **Modularidad**: Componentes intercambiables y extensibles

---

## ğŸ’» ESTÃNDARES DE CÃ“DIGO

### Nomenclatura
- **CÃ³digo**: InglÃ©s (variables, funciones, clases)
- **Comentarios**: EspaÃ±ol (docstrings, inline, docs)
- **Convenciones**: PEP 8 estricto

### Arquitectura
- **Paradigma**: ProgramaciÃ³n Orientada a Objetos
- **Type Hints**: Obligatorios en todas las funciones/mÃ©todos
- **Docstrings**: Formato Google/NumPy en espaÃ±ol

### Ejemplo de Estructura
```python
from typing import List, Optional, Tuple
from enum import Enum
from dataclasses import dataclass

class CharacterClass(Enum):
    """Clases de personajes disponibles en el juego."""
    WARRIOR = "warrior"
    MAGE = "mage"
    ROGUE = "rogue"
    TANK = "tank"
    SUPPORT = "support"

class StatusEffect(Enum):
    """Efectos de estado que pueden afectar a los personajes."""
    BURN = "burn"       # DaÃ±o por turno
    POISON = "poison"   # DaÃ±o creciente
    STUN = "stun"       # Pierde turno
    SHIELD = "shield"   # Reduce daÃ±o
    BUFF = "buff"       # Aumenta stats

@dataclass
class Stats:
    """
    EstadÃ­sticas base de un personaje.
    
    Attributes:
        hp: Puntos de vida mÃ¡ximos
        attack: Poder de ataque base
        defense: ReducciÃ³n de daÃ±o
        speed: Determina orden de turno
    """
    hp: int
    attack: int
    defense: int
    speed: int

class Character:
    """
    Representa un personaje del juego con sus atributos, habilidades y estado.
    
    Similar a un PokÃ©mon, cada personaje tiene una clase (tipo), estadÃ­sticas,
    y puede aprender habilidades especiales.
    
    Attributes:
        name: Nombre del personaje
        char_class: Clase del personaje (determina ventajas de tipo)
        stats: EstadÃ­sticas base (HP, ATK, DEF, SPD)
        current_hp: HP actual del personaje
        abilities: Lista de habilidades disponibles
        status_effects: Efectos de estado activos
        is_active: Si es el personaje activo en combate
    """
    
    def __init__(
        self, 
        name: str, 
        char_class: CharacterClass,
        stats: Stats,
        abilities: Optional[List['Ability']] = None
    ) -> None:
        """
        Inicializa un nuevo personaje.
        
        Args:
            name: Nombre Ãºnico del personaje
            char_class: Clase del enum CharacterClass
            stats: Objeto Stats con estadÃ­sticas base
            abilities: Habilidades especiales (mÃ¡x 4, como en PokÃ©mon)
            
        Raises:
            ValueError: Si stats tiene valores invÃ¡lidos o abilities > 4
        """
        if stats.hp <= 0:
            raise ValueError("HP debe ser positivo")
        if stats.attack < 0 or stats.defense < 0 or stats.speed < 0:
            raise ValueError("Las stats no pueden ser negativas")
        if abilities and len(abilities) > 4:
            raise ValueError("Un personaje puede tener mÃ¡ximo 4 habilidades")
            
        self.name = name
        self.char_class = char_class
        self.stats = stats
        self.current_hp = stats.hp
        self.abilities = abilities or []
        self.status_effects: List[StatusEffect] = []
        self.is_active = False
    
    def take_damage(self, damage: int) -> int:
        """
        Aplica daÃ±o al personaje considerando defensa.
        
        La fÃ³rmula de daÃ±o es similar a PokÃ©mon:
        damage_taken = max(1, damage - defense)
        
        Args:
            damage: Cantidad de daÃ±o base a aplicar
            
        Returns:
            Cantidad real de daÃ±o recibido despuÃ©s de defensa
        """
        # Calcular daÃ±o efectivo
        actual_damage = max(1, damage - self.stats.defense)
        
        # Aplicar daÃ±o
        self.current_hp = max(0, self.current_hp - actual_damage)
        
        return actual_damage
    
    def is_alive(self) -> bool:
        """Verifica si el personaje sigue en combate."""
        return self.current_hp > 0
    
    def heal(self, amount: int) -> int:
        """
        Restaura HP al personaje.
        
        Args:
            amount: Cantidad de HP a restaurar
            
        Returns:
            Cantidad real de HP restaurado (no puede exceder max HP)
        """
        old_hp = self.current_hp
        self.current_hp = min(self.stats.hp, self.current_hp + amount)
        return self.current_hp - old_hp
    
    def apply_status_effect(self, effect: StatusEffect) -> bool:
        """
        Aplica un efecto de estado al personaje.
        
        Args:
            effect: Efecto a aplicar
            
        Returns:
            True si se aplicÃ³ exitosamente, False si ya lo tenÃ­a
        """
        if effect not in self.status_effects:
            self.status_effects.append(effect)
            return True
        return False
    
    def __repr__(self) -> str:
        return (f"Character({self.name}, {self.char_class.value}, "
                f"HP: {self.current_hp}/{self.stats.hp})")

class Team:
    """
    Representa un equipo de 3 personajes.
    
    Similar al sistema de PokÃ©mon, solo un personaje puede estar activo
    a la vez, pero el jugador puede cambiar entre ellos.
    
    Attributes:
        characters: Lista de 3 personajes
        active_index: Ãndice del personaje activo (0-2)
    """
    
    def __init__(self, characters: List[Character]) -> None:
        """
        Inicializa un equipo.
        
        Args:
            characters: Lista de exactamente 3 personajes
            
        Raises:
            ValueError: Si no hay exactamente 3 personajes
        """
        if len(characters) != 3:
            raise ValueError("Un equipo debe tener exactamente 3 personajes")
            
        self.characters = characters
        self.active_index = 0
        self.characters[0].is_active = True
    
    @property
    def active_character(self) -> Character:
        """Retorna el personaje activo actual."""
        return self.characters[self.active_index]
    
    def switch_character(self, new_index: int) -> bool:
        """
        Cambia el personaje activo.
        
        Args:
            new_index: Ãndice del nuevo personaje activo (0-2)
            
        Returns:
            True si el cambio fue exitoso, False si no es vÃ¡lido
        """
        if new_index < 0 or new_index >= 3:
            return False
        if new_index == self.active_index:
            return False  # Ya es el activo
        if not self.characters[new_index].is_alive():
            return False  # No puede cambiar a personaje derrotado
            
        # Realizar cambio
        self.characters[self.active_index].is_active = False
        self.active_index = new_index
        self.characters[self.active_index].is_active = True
        return True
    
    def is_defeated(self) -> bool:
        """Verifica si el equipo ha sido completamente derrotado."""
        return all(not char.is_alive() for char in self.characters)
    
    def get_alive_characters(self) -> List[Character]:
        """Retorna lista de personajes vivos."""
        return [char for char in self.characters if char.is_alive()]
    
    def __repr__(self) -> str:
        return f"Team([{', '.join(str(c) for c in self.characters)}])"
```

---

## ğŸ§© ARQUITECTURA DEL PROYECTO

```
battlerpg_ai/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/              # LÃ³gica fundamental del juego
â”‚   â”‚   â”œâ”€â”€ character.py   # Sistema de personajes
â”‚   â”‚   â”œâ”€â”€ team.py        # GestiÃ³n de equipos
â”‚   â”‚   â”œâ”€â”€ ability.py     # Habilidades especiales
â”‚   â”‚   â”œâ”€â”€ stats.py       # EstadÃ­sticas y tipos
â”‚   â”‚   â””â”€â”€ player.py      # Jugadores base
â”‚   â”œâ”€â”€ engine/            # Motor del juego
â”‚   â”‚   â”œâ”€â”€ battle_state.py      # Estado del combate
â”‚   â”‚   â”œâ”€â”€ turn_manager.py      # GestiÃ³n de turnos
â”‚   â”‚   â”œâ”€â”€ action_validator.py  # ValidaciÃ³n de acciones
â”‚   â”‚   â”œâ”€â”€ damage_calculator.py # CÃ¡lculo de daÃ±o
â”‚   â”‚   â”œâ”€â”€ type_system.py       # Ventajas de tipo
â”‚   â”‚   â””â”€â”€ victory_checker.py   # Condiciones de victoria
â”‚   â”œâ”€â”€ ai/                # Agentes de IA
â”‚   â”‚   â”œâ”€â”€ heuristic/     # IA basada en heurÃ­sticas
â”‚   â”‚   â”‚   â”œâ”€â”€ basic_player.py
â”‚   â”‚   â”‚   â”œâ”€â”€ type_advantage_player.py
â”‚   â”‚   â”‚   â””â”€â”€ aggressive_player.py
â”‚   â”‚   â”œâ”€â”€ reinforcement/ # Agentes RL
â”‚   â”‚   â”‚   â”œâ”€â”€ q_learning_agent.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dqn_agent.py
â”‚   â”‚   â”‚   â””â”€â”€ state_encoder.py
â”‚   â”‚   â””â”€â”€ behavior/      # ObservaciÃ³n de comportamiento
â”‚   â”‚       â”œâ”€â”€ behavior_tracker.py
â”‚   â”‚       â”œâ”€â”€ opponent_profile.py
â”‚   â”‚       â””â”€â”€ pattern_detector.py
â”‚   â”œâ”€â”€ training/          # Sistema de entrenamiento
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ data_collector.py
â”‚   â”‚   â””â”€â”€ replay_buffer.py
â”‚   â””â”€â”€ utils/             # Utilidades
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ visualizer.py
â”œâ”€â”€ tests/                 # Tests unitarios
â”‚   â”œâ”€â”€ test_character.py
â”‚   â”œâ”€â”€ test_team.py
â”‚   â”œâ”€â”€ test_battle.py
â”‚   â””â”€â”€ test_ai/
â”œâ”€â”€ notebooks/             # AnÃ¡lisis y visualizaciones
â”œâ”€â”€ configs/               # Archivos de configuraciÃ³n
â”‚   â”œâ”€â”€ characters.json    # Definiciones de personajes
â”‚   â”œâ”€â”€ abilities.json     # Definiciones de habilidades
â”‚   â””â”€â”€ training_config.yaml
â””â”€â”€ docs/                  # DocumentaciÃ³n adicional
```

---

## ğŸ”§ FLUJO DE TRABAJO

### Para Cada Nueva Feature:

1. **AnÃ¡lisis**
   - Revisar documentos del proyecto relevantes
   - Verificar cÃ³digo existente en `/src`
   - Identificar dependencias y conflictos potenciales

2. **DiseÃ±o**
   - Proponer arquitectura de clases (UML si es complejo)
   - Justificar decisiones de diseÃ±o
   - Listar trade-offs y alternativas consideradas

3. **ImplementaciÃ³n**
   - CÃ³digo modular con responsabilidades claras
   - Type hints completos
   - Docstrings detallados en espaÃ±ol
   - ValidaciÃ³n de inputs defensiva
   - Manejo de errores con try-except

4. **Testing**
   - Tests unitarios con pytest para lÃ³gica crÃ­tica
   - Casos edge incluidos
   - Coverage > 80% para mÃ³dulos core

5. **DocumentaciÃ³n**
   - README actualizado si aplica
   - Comentarios inline en lÃ³gica compleja
   - Diagramas para flujos no triviales

### Pregunta Antes de Asumir
- Si la especificaciÃ³n es ambigua
- Si hay mÃºltiples formas razonables de implementar algo
- Si necesitas mÃ¡s contexto sobre decisiones previas

---

## ğŸ“ CONCEPTOS TÃ‰CNICOS CLAVE

### MecÃ¡nicas de Combate (Inspirado en PokÃ©mon)

#### Sistema de Turnos
- **Velocidad determina orden**: Personaje mÃ¡s rÃ¡pido ataca primero
- **Cambios son prioritarios**: Cambiar personaje ocurre antes que ataques
- **Prioridad de habilidades**: Algunas habilidades tienen prioridad alta

#### Ventajas de Tipo (Sistema Rock-Paper-Scissors Extendido)
```
WARRIOR > ROGUE > MAGE > WARRIOR (ciclo bÃ¡sico)
TANK: Resistente a WARRIOR y ROGUE
SUPPORT: Neutral contra todos, buffs aliados

Multiplicadores:
- SÃºper efectivo: 1.5x daÃ±o
- Normal: 1.0x daÃ±o
- No muy efectivo: 0.5x daÃ±o
```

#### Acciones Disponibles
1. **ATTACK**: Ataque bÃ¡sico con daÃ±o base del personaje
2. **USE_ABILITY**: Usa una habilidad especial (daÃ±o, buff, debuff, heal)
3. **SWITCH**: Cambia al siguiente personaje del equipo
4. **ITEM** (opcional para MVP): Usa objeto (pociÃ³n, revivir)

#### FÃ³rmula de DaÃ±o BÃ¡sica
```python
# DaÃ±o base
base_damage = attacker.attack

# Modificador de tipo
type_multiplier = get_type_effectiveness(attacker.char_class, defender.char_class)

# DaÃ±o con tipo
typed_damage = base_damage * type_multiplier

# Aplicar defensa
final_damage = max(1, typed_damage - defender.defense)

# Factor aleatorio (Â±10% variaciÃ³n)
actual_damage = final_damage * random.uniform(0.9, 1.1)
```

### Reinforcement Learning

#### Espacio de Estado (State Space)
RepresentaciÃ³n vectorial del combate:
```python
state = [
    # Jugador 1
    char1_hp_ratio,      # HP actual / HP mÃ¡ximo
    char2_hp_ratio,
    char3_hp_ratio,
    active_char_index,   # 0, 1, o 2
    
    # Jugador 2 (oponente)
    opp_char1_hp_ratio,
    opp_char2_hp_ratio,
    opp_char3_hp_ratio,
    opp_active_char_index,
    
    # Ventaja de tipo
    type_advantage,      # -1, 0, +1
    
    # Efectos de estado (one-hot)
    has_burn, has_poison, has_stun, has_shield, has_buff
]
```

#### Espacio de AcciÃ³n (Action Space)
```python
actions = [
    0: ATTACK,
    1: USE_ABILITY_1,
    2: USE_ABILITY_2,
    3: USE_ABILITY_3,
    4: USE_ABILITY_4,
    5: SWITCH_TO_CHAR_1,
    6: SWITCH_TO_CHAR_2,
    7: SWITCH_TO_CHAR_3
]
```

#### FunciÃ³n de Recompensa
```python
reward = 0

# Recompensas inmediatas
reward += damage_dealt * 0.01        # PequeÃ±a recompensa por daÃ±o
reward -= damage_received * 0.01     # PenalizaciÃ³n por recibir daÃ±o

# Recompensas por knockout
if enemy_character_fainted:
    reward += 1.0
if own_character_fainted:
    reward -= 1.0

# Recompensa por victoria/derrota
if battle_won:
    reward += 10.0
elif battle_lost:
    reward -= 10.0

# Recompensa por eficiencia (bonus por ganar rÃ¡pido)
if battle_won:
    reward += max(0, (50 - num_turns) * 0.1)
```

### Algoritmos RL a Implementar

#### 1. Q-Learning (MVP)
- **Ventajas**: Simple, interpretable, bueno para espacios pequeÃ±os
- **Desventajas**: Requiere discretizaciÃ³n del espacio de estado
- **Uso**: Primer agente RL bÃ¡sico

#### 2. Deep Q-Network (DQN) (Post-MVP)
- **Ventajas**: Maneja espacios continuos, mÃ¡s flexible
- **Desventajas**: MÃ¡s complejo, requiere mÃ¡s entrenamiento
- **Uso**: VersiÃ³n mejorada del agente

#### 3. Policy Gradient (Opcional)
- **Ventajas**: Aprende polÃ­ticas estocÃ¡sticas, mejor para exploraciÃ³n
- **Desventajas**: Alta varianza, convergencia lenta
- **Uso**: ExperimentaciÃ³n avanzada

---

## ğŸ“Š MÃ‰TRICAS DE COMPORTAMIENTO DEL OPONENTE

### Patrones a Detectar

#### 1. Agresividad
```python
aggression_score = (
    num_attacks / total_actions
)
# 0.0 = pasivo, 1.0 = muy agresivo
```

#### 2. Frecuencia de Cambio
```python
switch_frequency = (
    num_switches / total_actions
)
# > 0.3 = cambia mucho
# < 0.1 = casi nunca cambia
```

#### 3. Uso de Habilidades
```python
ability_usage_rate = (
    num_abilities_used / total_actions
)
# Por tipo de habilidad (damage, buff, debuff, heal)
```

#### 4. Respuesta a HP Bajo
```python
# Â¿Cambia cuando HP < 30%?
low_hp_switch_tendency = (
    switches_when_hp_low / times_hp_was_low
)
```

#### 5. Aprovechamiento de Tipo
```python
# Â¿Mantiene ventaja de tipo?
type_advantage_awareness = (
    attacks_with_advantage / total_attacks
)
```

### OpponentProfile
```python
@dataclass
class OpponentProfile:
    """
    Perfil de comportamiento del oponente.
    
    Se actualiza tras cada partida para detectar patrones.
    """
    aggression: float           # 0.0 - 1.0
    switch_frequency: float     # 0.0 - 1.0
    ability_preference: Dict[str, float]  # Por tipo de habilidad
    low_hp_behavior: str        # "aggressive", "defensive", "switch"
    type_awareness: float       # 0.0 - 1.0
    
    num_games_observed: int
    total_actions: int
```

---

## âš ï¸ CONSIDERACIONES ESPECIALES

### Balance del Juego
- **Evitar estrategias dominantes**: Ninguna clase debe ganar siempre
- **Ventajas de tipo balanceadas**: Multiplicadores razonables (1.5x max)
- **Habilidades equilibradas**: Cooldowns o costos para habilidades potentes
- **Testing extensivo**: Simular 1000+ partidas entre diferentes estrategias

### Performance
- **VectorizaciÃ³n**: Usar NumPy para cÃ¡lculos batch en entrenamiento
- **Replay Buffer**: Almacenar experiencias para training off-policy
- **ParalelizaciÃ³n**: MÃºltiples batallas simultÃ¡neas durante entrenamiento
- **Early stopping**: Detener entrenamiento si converge

### Reproducibilidad
- **Seeds fijos**: `random.seed()`, `np.random.seed()`, `torch.manual_seed()`
- **Logging completo**: HiperparÃ¡metros, arquitectura, resultados
- **Versionado de modelos**: Guardar checkpoints cada N episodios

### Debugging
- **VisualizaciÃ³n de partidas**: Imprimir log detallado de cada turno
- **MÃ©tricas en tiempo real**: Winrate, avg reward, loss (si DQN)
- **AnÃ¡lisis de polÃ­ticas**: Â¿QuÃ© acciones prefiere el agente?

---

## ğŸš€ CRITERIOS DE Ã‰XITO DEL MVP

### Funcionalidad Core
- âœ… Dos jugadores pueden completar una batalla con equipos de 3 personajes
- âœ… Sistema de turnos basado en velocidad funciona correctamente
- âœ… Ventajas de tipo se aplican correctamente
- âœ… Cambios de personaje funcionan como se espera
- âœ… Victoria se detecta cuando un equipo es eliminado

### IA HeurÃ­stica
- âœ… HeuristicPlayer toma decisiones razonables:
  - Cambia si hay desventaja de tipo significativa
  - Ataca personajes con HP bajo
  - Usa habilidades en momentos apropiados
- âœ… RandomPlayer sirve como baseline funcional
- âœ… HeuristicPlayer gana >70% vs RandomPlayer

### ObservaciÃ³n de Comportamiento
- âœ… BehaviorTracker registra todas las acciones correctamente
- âœ… Se calculan mÃ©tricas bÃ¡sicas (agresividad, frecuencia de cambio)
- âœ… OpponentProfile se actualiza tras cada partida

### Reinforcement Learning Inicial
- âœ… Agente RL puede entrenarse contra HeuristicPlayer
- âœ… Performance mejora mediblemente tras 1000+ episodios
- âœ… Winrate aumenta de ~30% â†’ ~50%+ durante entrenamiento
- âœ… Aprende al menos 1 patrÃ³n adversarial observable:
  - Ejemplo: Detecta que oponente es agresivo â†’ juega mÃ¡s defensivo
  - Ejemplo: Detecta que oponente cambia poco â†’ explota ventaja de tipo

---

## ğŸ“ TEMPLATE DE RESPUESTA

Cuando implementes una feature, estructura tu respuesta asÃ­:

### 1. ğŸ¯ Objetivo
[QuÃ© vamos a implementar y por quÃ©]

### 2. ğŸ—ï¸ DiseÃ±o
[Arquitectura propuesta, clases, interacciones]
[Diagrama UML si es complejo]

### 3. ğŸ’¡ Decisiones Clave
[JustificaciÃ³n de elecciones tÃ©cnicas]
[Trade-offs considerados]

### 4. ğŸ’» ImplementaciÃ³n
[CÃ³digo paso a paso con explicaciones]

### 5. ğŸ§ª Testing
[Tests unitarios si es componente crÃ­tico]

### 6. ğŸ“Š VisualizaciÃ³n (Opcional)
[GrÃ¡ficos o diagramas si ayudan a entender]

### 7. ğŸ”„ PrÃ³ximos Pasos
[QuÃ© falta o cÃ³mo extender esta feature]

---

## ğŸ¯ COMANDOS ÃšTILES PARA EL AGENTE

### Antes de Implementar
```
"Revisa los archivos en /src/core para ver la implementaciÃ³n actual"
"Explica brevemente cÃ³mo funciona el sistema de ventajas de tipo antes de codificar"
"Â¿QuÃ© trade-offs hay entre usar una tabla de ventajas vs un sistema calculado?"
```

### Durante ImplementaciÃ³n
```
"Implementa la clase Character con type hints y docstrings en espaÃ±ol"
"AÃ±ade tests unitarios para el mÃ©todo take_damage()"
"Crea un diagrama de flujo para el sistema de turnos"
```

### Debugging y OptimizaciÃ³n
```
"Analiza por quÃ© el agente RL no estÃ¡ mejorando su winrate"
"Sugiere visualizaciones para las mÃ©tricas de entrenamiento"
"Optimiza el StateEncoder para reducir dimensionalidad"
```

---

## ğŸ® FASE ACTUAL: MVP - FUNDAMENTOS DEL JUEGO

**Prioridad CRÃTICA**: Implementar el sistema bÃ¡sico de personajes, equipos y combate.

**Orden de ImplementaciÃ³n Sugerido**:
1. `core/character.py` - Sistema de personajes base
2. `core/team.py` - GestiÃ³n de equipos de 3 personajes
3. `core/ability.py` - Habilidades especiales
4. `engine/type_system.py` - Ventajas de tipo
5. `engine/damage_calculator.py` - CÃ¡lculo de daÃ±o
6. `engine/battle_state.py` - Estado del combate
7. `engine/turn_manager.py` - GestiÃ³n de turnos
8. `core/player.py` - Jugadores base (Random, Heuristic)

**Siguiente Tarea EspecÃ­fica**: 
Espera instrucciones del usuario sobre quÃ© componente implementar primero.

---

## ğŸ”¥ RECORDATORIOS CRÃTICOS

1. **SIEMPRE cÃ³digo en inglÃ©s, comentarios en espaÃ±ol**
2. **SIEMPRE incluir type hints completos**
3. **SIEMPRE docstrings en espaÃ±ol con formato Google/NumPy**
4. **SIEMPRE validar inputs antes de procesarlos**
5. **SIEMPRE preguntar si algo no estÃ¡ claro**
6. **NUNCA asumir cuando hay ambigÃ¼edad**
7. **NUNCA omitir manejo de errores en cÃ³digo de producciÃ³n**
8. **SIEMPRE tests para lÃ³gica crÃ­tica del juego**

---

## ğŸ“š RECURSOS ADICIONALES

### Referencias de PokÃ©mon (InspiraciÃ³n)
- Sistema de tipos y ventajas
- MecÃ¡nica de cambio de personajes
- FÃ³rmulas de daÃ±o balanceadas
- Prioridad de movimientos

### Referencias de RL
- Sutton & Barto - Reinforcement Learning: An Introduction
- OpenAI Gym - Entornos de entrenamiento
- Stable Baselines3 - Implementaciones de referencia

### Herramientas Recomendadas
- **Desarrollo**: VSCode con Python extension
- **Testing**: pytest, pytest-cov
- **RL**: NumPy, PyTorch (si DQN)
- **VisualizaciÃ³n**: matplotlib, seaborn
- **Logging**: Python logging module, TensorBoard (opcional)

---

**Â¡Listo para comenzar el desarrollo! Espero tus instrucciones sobre quÃ© implementar primero.**
